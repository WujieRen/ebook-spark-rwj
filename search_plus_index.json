{"./":{"url":"./","title":"Introduction","keywords":"","body":"Introduction Apache Spark是用于大规模数据处理的统一分析引擎。它提供Java、Scala、Python和R的高级API，以及支持通用执行图的优化引擎。他还提供了一系列高级的工具，包括用于SQL和结构化数据的处理的Spark SQL，用于机器学习的MLib，用于图形处理的GraphX，以及用于增量计算和流处理的Spark Streaming。 Spark和MapReduce异同？ 两者都是分布式计算框架。Spark借鉴并沿用了MapReduce的一些东西，但是对MapReduce做了大量的优化。 MapReduce主要可分为map和reduce。map是在不同的机器上独立且同步运行的，它的主要目的是将数据转换为key-value的形式；而reduce是做聚合运算，它也是在不同的机器上独立且同步运行。map和reduce两者之间夹杂着一步数据移动（shuffle），该操作涉及数量巨大的网络传输（network I/O），需要耗费大量时间。而且一个MapReduce任务只能包含一次Map和一次Reduce，计算完成后的结果会写到磁盘上供下次使用。如果运算包含大量循环或者需要多个MapReduce任务，那么整个运算过程中网络IO和磁盘IO会耗费大量的时间。 而Spark主要改进的就是这点。Spark延续了MapReduce的设计思路：对数据的计算也分为Map和Reduce两类，但不同的是，一个Spark任务不止包含一个Map和一个Reduce，而是由一系列的Map、Reduce构成。而且计算的中间结果可以高效地传给下一个计算步骤，这一系列过程都在内存中完成。相比较MapReduce而言，它的性能提升了10~100倍。 另外，MapReduce编码较为死板，而Spark则提供了大量易用的transaction和action API，极大提升了开发效率。 Apache Spark的特性 快速：Apache Spark使用最新的DAG调度程序，查询优化器和物理执行引擎，为批处理和流数据提供了高性能。 易用；Spark提供了80多个高级操作方法，以便轻松地构建并行应用程序；可以交互使用用Java、Scala、Python、R和SQL快速实现。 通用性：Spark支持一系列组件库包括SQL和DataFrames，用于机器学习的MLib，GraphX，以及SparkStreaming。您可以在同一应用程序中无缝混合使用这些库。 丰富数据源：您可以在EC2，Hadoop YARN，Mesos或者Kubernates上运行Spark。访问存储在HDFS，Alluxio，Apache Cassendra，Apache HBase，Apache Hive以及其他数据源上的数据。 By renwujie            updated 2021-02-19 22:11:25 "}}